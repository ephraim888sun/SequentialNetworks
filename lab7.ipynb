{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lab Assignment Seven: Sequential Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name 1: Jadon Swearingen (5000)\n",
    "Name 2: Ephraim Sun (7000)\n",
    "Name 3: Adeeb Abdul Taher (7000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will select a prediction task to perform on your dataset, evaluate a sequential architecture and tune hyper-parameters. If any part of the assignment is not clear, ask the instructor to clarify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Selection\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/rajatkumar30/fake-news\n",
    "\n",
    "We have chosen a fake news dataset where given the title and text of the article, we should predict if the news is fake or real. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed). Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). Discuss methods of tokenization in your dataset as well as any decisions to force a specific length of sequence.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import nltk, re, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Flatten, Dropout, Layer\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "mpl.style.use(\"seaborn-v0_8-deep\")\n",
    "mpl.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"lines.linewidth\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "df = pd.read_csv(\"./dataset/news.csv\")\n",
    "\n",
    "# Load less data\n",
    "df =df.head(1000)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Data\n",
    "\n",
    "This function performs several preprocessing steps on the input DataFrame:\n",
    "1. Converts the 'title' and 'text' columns to string type to ensure consistency.\n",
    "2. Encodes the 'label' column, mapping 'FAKE' to 1 and 'REAL' to 0, to prepare for machine learning algorithms.\n",
    "3. Drops the 'Unnamed: 0' column which is often an artifact from reading files with an index.\n",
    "4. Combine the article text + article together\n",
    "5. Clean the text by lowercasing everything, removing stopwords, and removing non-alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(txt):\n",
    "      \n",
    "    #Creates list of possible stopwords from nltk library\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    # Lowercase\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    txt = ' '.join([word for word in txt.split() if word not in (stop)])\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    txt = re.sub('[^a-z]',' ',txt)\n",
    "    return txt  \n",
    "\n",
    "\n",
    "def pre_process(df):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - df (pd.Dataframe): The DataFrame to be preprocessed\n",
    "\n",
    "    Returns: \n",
    "    - df (pd.DataFrame): The DataFrame after preprocessed\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    columns = [\"title\", \"text\"]\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str)  # Convert each column to string\n",
    "\n",
    "    # Map 'label' values from 'FAKE'/'REAL' to 1/0\n",
    "    df[\"label\"] = df[\"label\"].map({\"FAKE\": 1, \"REAL\": 0})\n",
    "\n",
    "    # Remove the 'Unnamed: 0' column, if it exists, as it's usually an artifact\n",
    "    df = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "\n",
    "    # Combining text and title for creating full news article with headline\n",
    "    df['final_news'] = df['title'] + \" \" + df['text']\n",
    "\n",
    "    # Cleaning the text\n",
    "    df['final_news'] = df['final_news'].apply(cleanText)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_characters(df, col_name):\n",
    "    \"\"\"\n",
    "    Adds a new column to the DataFrame containing the number of characters in each message.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame. The DataFrame to which the new column will be added.\n",
    "    - col_name: str. The name of the column containing the messages.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame. The original DataFrame with a new column added.\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"Characters\"] = df[col_name].apply(len)\n",
    "    return df\n",
    "\n",
    "\n",
    "def num_sentences(df, col_name):\n",
    "    \"\"\"\n",
    "    Adds a new column to the DataFrame containing the number of sentences in each message.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame. The DataFrame to which the new column will be added.\n",
    "    - col_name: str. The name of the column containing the messages.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame. The original DataFrame with a new column added.\n",
    "    \"\"\"\n",
    "    df[\"Sentences\"] = df[col_name].apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def num_words(df, col_name):\n",
    "    \"\"\"\n",
    "    Adds a new column to the DataFrame containing the number of words in each message.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame. The DataFrame to which the new column will be added.\n",
    "    - col_name: str. The name of the column containing the messages.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame. The original DataFrame with a new column added.\n",
    "    \"\"\"\n",
    "    df[\"Words\"] = df[col_name].apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_plot(df, col_name):\n",
    "    \"\"\"\n",
    "    Creates a pie plot to show the distribution of real and fake news in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame. The DataFrame containing the data.\n",
    "    - col_name: str. The name of the column containing the labels.\n",
    "\n",
    "    Returns:\n",
    "    - None. Displays a pie plot.\n",
    "    \"\"\"\n",
    "    sizes = df[\"label\"].value_counts()\n",
    "    labels = [\"Real\", \"Fake\"]\n",
    "    colors = [\"#ff9999\", \"#66b3ff\"]\n",
    "\n",
    "    plt.pie(\n",
    "        sizes,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        colors=colors,\n",
    "        startangle=90,\n",
    "        explode=(0.1, 0),\n",
    "        shadow=True,\n",
    "    )\n",
    "\n",
    "    plt.title(\"Distribution of Real and Fake News in \" + col_name)\n",
    "    plt.legend(labels, title=\"News Type\", loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_features(df, features, col_name):\n",
    "    \"\"\"\n",
    "    Plot distributions of specified features in a DataFrame, comparing 'Real' and 'Fake' news.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - features: List of strings, names of the columns for which to plot the distributions.\n",
    "\n",
    "    Each feature's distribution is plotted in a separate row, with 'Real' news in blue and 'Fake' news in pink.\n",
    "    \"\"\"\n",
    "    # Create a figure with subplots arranged in 3 rows and 1 column\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=len(features), ncols=1, figsize=(15, 6 * len(features))\n",
    "    )\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        # Plot distribution for 'Real' news\n",
    "        sns.histplot(\n",
    "            df[df[\"label\"] == 0][feature],\n",
    "            kde=True,\n",
    "            color=\"#66b3ff\",\n",
    "            label=\"Real\",\n",
    "            ax=axes[i],\n",
    "        )\n",
    "        # Plot distribution for 'Fake' news\n",
    "        sns.histplot(\n",
    "            df[df[\"label\"] == 1][feature],\n",
    "            kde=True,\n",
    "            color=\"#ff9999\",\n",
    "            label=\"Fake\",\n",
    "            ax=axes[i],\n",
    "        )\n",
    "        # Setting the title for each subplot\n",
    "        axes[i].set_title(f\"{feature} Distribution for {col_name} dataset\")\n",
    "        # Setting the labels for each subplot\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel(\"Count\")\n",
    "        # Adding legend to each subplot\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pre_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = num_characters(df, \"final_news\")\n",
    "df = num_sentences(df, \"final_news\")\n",
    "df = num_words(df, \"final_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_plot(df, \"Full Text Article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of the title column\n",
    "df_title = df\n",
    "\n",
    "df_title = num_characters(df_title, \"title\")\n",
    "df_title = num_sentences(df_title, \"title\")\n",
    "df_title = num_words(df_title, \"title\")\n",
    "\n",
    "plot_distribution_features(df_title, [\"Characters\", \"Sentences\", \"Words\"], \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of the text column\n",
    "df_text = df\n",
    "\n",
    "df_text = num_characters(df_text, \"text\")\n",
    "df_text = num_sentences(df_text, \"text\")\n",
    "df_text = num_words(df_text, \"text\")\n",
    "\n",
    "plot_distribution_features(df_text, [\"Characters\", \"Sentences\", \"Words\"], \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Representation for Model\n",
    "X = df['final_news'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Determine input length\n",
    "text_lengths = [len(seq) for seq in X]\n",
    "avg_length = np.mean(text_lengths)\n",
    "std_length = np.std(text_lengths)\n",
    "input_length = int(avg_length + std_length)\n",
    "\n",
    "\n",
    "# Pad sequences\n",
    "input_length = max(len(seq) for seq in X)\n",
    "X = pad_sequences(X, maxlen=input_length)\n",
    "\n",
    "# Determine vocabulary size\n",
    "input_dim = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Define variables\n",
    "output_dim = 100  # Modify this as per your requirements\n",
    "params = (input_dim, output_dim, input_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided on the following metrics\n",
    "\n",
    "Precision: Precision is the ratio of true positive predictions (correctly identifying fake news) to the total number of positive predictions (both true positives and false positives). Precision indicates the algorithm's ability to correctly identify fake news without mistakenly classifying real news as fake. In the context of fake news detection, precision is crucial because incorrectly labeling legitimate news as fake can have serious consequences, including reputational damage to news sources and misinformation propagation.\n",
    "\n",
    "Recall (Sensitivity): Recall is the ratio of true positive predictions to the total number of actual positive instances (both true positives and false negatives). Recall measures the algorithm's ability to correctly identify all instances of fake news in the dataset. A high recall indicates that the algorithm can effectively detect most of the fake news samples, minimizing the risk of allowing misinformation to spread.\n",
    "\n",
    "F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balanced assessment of the algorithm's performance by taking both precision and recall into account. The F1 score is particularly useful when there is an uneven distribution of classes in the dataset, as it considers false positives and false negatives.\n",
    "\n",
    "Specificity: Specificity is the ratio of true negative predictions (correctly identifying real news) to the total number of actual negative instances (both true negatives and false positives). Specificity measures the algorithm's ability to correctly identify real news without mistakenly classifying it as fake. While precision focuses on the positive class (fake news), specificity provides insights into the algorithm's performance in accurately identifying the negative class (real news).\n",
    "\n",
    "Area Under the Receiver Operating Characteristic Curve (AUC-ROC): The AUC-ROC is a widely used metric that evaluates the algorithm's overall performance across different threshold settings. It measures the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) at various classification thresholds. A higher AUC-ROC score indicates better overall performance, considering both true positives and true negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your train/test splitting method is a realistic mirroring of how an algorithm would be used in practice. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to utilize Stratified 10-fold Cross-Validation:\n",
    "\n",
    "\n",
    "Handling Class Imbalance: Since your dataset contains an equal distribution of fake news and real news samples, using stratified sampling ensures that each fold of the cross-validation maintains the same proportion of both classes. This is particularly important when dealing with imbalanced datasets, where one class is significantly more prevalent than the other. By maintaining class balance in each fold, the evaluation becomes more representative of the model's performance on unseen data.\n",
    "\n",
    "Robust Performance Evaluation: Cross-validation provides a more robust estimate of your model's performance by iteratively training and testing the model on different subsets of the data. In Stratified 10-fold Cross-Validation, the dataset is divided into ten equal parts (folds), with each fold serving as the testing set once while the remaining nine folds are used for training. By averaging the performance metrics across all folds, you obtain a more reliable assessment of the model's generalization ability.\n",
    "\n",
    "Mitigating Overfitting: Cross-validation helps in detecting and mitigating overfitting issues. Overfitting occurs when a model performs well on the training data but fails to generalize to unseen data. With Stratified 10-fold Cross-Validation, the model is exposed to multiple training and testing subsets. If the model consistently performs well across all folds, it indicates that it has learned patterns that are generalizable and not specific to a particular subset.\n",
    "\n",
    "Optimal Hyperparameter Tuning: Cross-validation is commonly used for hyperparameter tuning. It allows you to evaluate different sets of hyperparameters on the training folds and select the best combination based on the average performance across the testing folds. This helps in finding the optimal hyperparameters that yield the best performance on unseen data.\n",
    "\n",
    "Realistic Mirroring of Model Usage: Stratified 10-fold Cross-Validation provides a realistic reflection of how the model would be used in practice. It simulates the scenario where the model is trained on a large dataset and tested on unseen data. This approach is especially useful when you don't have a separate validation or holdout dataset for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3 points] Investigate at least two different sequential network architectures (e.g., a CNN and a Transformer). Alternatively, you may also choose a recurrent network and Transformer network. Be sure to use an embedding layer (try to use a pre-trained embedding, if possible). Adjust one hyper-parameter of each network to potentially improve generalization performance (train a total of at least four models). Visualize the performance of training and validation sets versus the training iterations, showing that the models converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN Model\n",
    "def rnn_model(input_dim, output_dim, input_length):\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(input_dim, output_dim, input_length=input_length),\n",
    "        \n",
    "        # LSTM layers\n",
    "        SimpleRNN(128, return_sequences=True),\n",
    "        SimpleRNN(64),\n",
    "        \n",
    "        # Dense output layer\n",
    "        Dense(1, activation='sigmoid')  # Dense layer with 1 unit and sigmoid activation for binary classification\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "def lstm_model(input_dim, output_dim, input_length):\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(input_dim, output_dim, input_length=input_length),\n",
    "        \n",
    "        # LSTM layers\n",
    "        LSTM(64, return_sequences=True),  # LSTM layer with 64 units and returns sequences\n",
    "        LSTM(32),  # LSTM layer with 32 units\n",
    "        \n",
    "        # Dense output layer\n",
    "        Dense(1, activation='sigmoid')  # Dense layer with 1 unit and sigmoid activation for binary classification\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model\n",
    "def gru_model(input_dim, output_dim, input_length):\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(input_dim, output_dim, input_length=input_length),\n",
    "        \n",
    "        # GRU layers\n",
    "        GRU(128, return_sequences=True),  # GRU layer with 128 units and returns sequences\n",
    "        GRU(64),  # GRU layer with 64 units\n",
    "        \n",
    "        # Dense output layer\n",
    "        Dense(1, activation='sigmoid')  # Dense layer with 1 unit and sigmoid activation for binary classification\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Taken from https://github.com/eclarson/MachineLearningNotebooks/blob/master/13a.%20Sequence%20Basics%20%5Bexperimental%5D.ipynb\n",
    "\n",
    "# The transformer architecture \n",
    "class TransformerBlock(Layer): # inherit from Keras Layer\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "        super().__init__()\n",
    "        # setup the model heads and feedforward network\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, \n",
    "                                      key_dim=embed_dim)\n",
    "        \n",
    "        # make a two layer network that processes the attention\n",
    "        self.ffn = Sequential()\n",
    "        self.ffn.add( Dense(ff_dim, activation='relu') )\n",
    "        self.ffn.add( Dense(embed_dim) )\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        # apply the layers as needed (similar to PyTorch)\n",
    "        \n",
    "        # get the attention output from multi heads\n",
    "        # Using same inpout here is self-attention\n",
    "        # call inputs are (query, value, key) \n",
    "        # if only two inputs given, value and key are assumed the same\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        \n",
    "        # create residual output, with attention\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out1 = self.dropout1(out1, training=training)\n",
    "        \n",
    "        # place through feed forward after layer norm\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out2 = self.dropout2(out2, training=training)\n",
    "        #return the residual from Dense layer\n",
    "        return out2\n",
    "    \n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # create two embeddings \n",
    "        # one for processing the tokens (words)\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, \n",
    "                                   output_dim=embed_dim)\n",
    "        # another embedding for processing the position\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, \n",
    "                                 output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # create a static position measure (input)\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # positions now goes from 0 to 500 (for IMdB) by 1\n",
    "        positions = self.pos_emb(positions)# embed these positions\n",
    "        x = self.token_emb(x) # embed the tokens\n",
    "        return x + positions # add embeddngs to get final embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Architecture Model\n",
    "def transformer_model(input_dim, output_dim, input_length):\n",
    "    embed_dim = 32  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 32  # Hidden layer size in the feed-forward network inside the transformer\n",
    "\n",
    "    inputs = Input(shape=(input_length,))\n",
    "    x = TokenAndPositionEmbedding(input_length, input_dim, embed_dim)(inputs)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(20, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(output_dim, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, epochs=20):\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model and store the training history\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "    # Return the training history and the trained model\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy and loss\n",
    "def plot_history(history, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all model variations\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "model_variations = [\n",
    "    (transformer_model, params),\n",
    "    (rnn_model, params),\n",
    "    (lstm_model, params),\n",
    "    (gru_model, params),\n",
    "]\n",
    "histories = []\n",
    "models = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print(\"Fold Number:\", len(histories) + 1)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    sub_histories = []\n",
    "    sub_models = []\n",
    "    for model_func, variation in model_variations:\n",
    "        model = model_func(*variation)\n",
    "        history, model = train_and_evaluate_model(model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, epochs=epochs)\n",
    "        sub_histories.append(history)\n",
    "        sub_models.append(model)\n",
    "\n",
    "    histories.append(sub_histories)\n",
    "    models.append(sub_models)\n",
    "\n",
    "# Visualize the performance of all model variations\n",
    "for idx, (model_func, variation) in enumerate(model_variations):\n",
    "    title = f\"{model_func.__name__} Variation {variation}\"\n",
    "    for i, history in enumerate(histories):\n",
    "        plot_history(history[i][idx], title=f\"{title} - Fold {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 points] Using the best parameters and architecture from the Transformer in the previous step, add a second Multi-headed self attention layer to your network. That is, the input to the second attention layer should be the output sequence of the first attention layer.  Visualize the performance of training and validation sets versus the training iterations, showing that the model converged.. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2 points] Use the method of train/test splitting and evaluation criteria that you argued for at the beginning of the lab. Visualize the results of all the models you trained.  Use proper statistical comparison techniques to determine which method(s) is (are) superior.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pre-trained ConceptNet Numberbatch embedding and compare to pre-trained GloVe. Which method is better for your specific application? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for pre-trained ConceptNet Numberbatch embedding and pre-trained GloVe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
